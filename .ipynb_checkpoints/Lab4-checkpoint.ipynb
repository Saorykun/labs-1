{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7892a287",
   "metadata": {},
   "source": [
    "- Прочитайте главы 12-14 из книги [Spark: The Definitive Guide](https://analyticsdata24.files.wordpress.com/2020/02/spark-the-definitive-guide40www.bigdatabugs.com_.pdf#%5B%7B%22num%22%3A484%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C792%2Cnull%5D).\n",
    "- Прочитайте главу 2 из книги [Hadoop: The Definitive Guide](https://grut-computing.com/HadoopBook.pdf#%5B%7B%22num%22%3A348%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2Cnull%2C588.58502%2Cnull%5D).\n",
    "- Выполните задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fa85fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/22 23:28:50 WARN Utils: Your hostname, student-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/12/22 23:28:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/22 23:28:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/22 23:28:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/22 23:28:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/22 23:29:00 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:191)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:851)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:893)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:261)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:50)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:314)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:245)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Ulysses, by James Joyce'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Lab4\").getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "textFiles = sc.textFile('hdfs://localhost:9000//user/student/gutenberg/*')\n",
    "\n",
    "textFiles.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5d53c",
   "metadata": {},
   "source": [
    "№0: Откройте каталог \"/home/student/Work/Labs/3.Hadoop/WordCount\" и изучите его содержимое.\n",
    "\n",
    "Здесь представлена реализация подсчета слов в текстовых документах по парадигме MapReduce для Apache Hadoop. В папке Input находятся текстовые файлы с входными данными. Они же загружены в файловую систему HDFS по адресу hdfs://localhost:9000//user/student/gutenberg. В папке Src представлен исходный код стадий map и reduce. В папке Scripts есть два скрипта: cluster_run.sh и local_run.sh. Первый выполняет подсчет слов на платформе Hadoop. Второй выполняет ту же задачу без Hadoop. В папку Output попадают результаты выполнения скриптов.\n",
    "\n",
    "Код ниже подсчитывает количество вхождений слов в текстовые документы с помощью платформы Apache Spark. Сравните этот код с реализацией той же задачи для Apache Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c320f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Project', 185),\n",
       " ('of', 23950),\n",
       " ('Ulysses,', 2),\n",
       " ('James', 30),\n",
       " ('Joyce', 3),\n",
       " ('', 34559)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textFiles\\\n",
    "    .flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "counts = words\\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "counts.take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effa31e",
   "metadata": {},
   "source": [
    "№1: Найдите количество вхождений только тех слов, длина которых не менее 5 символов. Выведите их в порядке убывания.\n",
    "\n",
    "[('which', 3637),\n",
    " ('their', 1691),\n",
    " ('there', 1224),\n",
    " ('other', 954),\n",
    " ('would', 884),\n",
    " ('these', 846)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a398f03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(641, 'between'),\n",
       " (600, 'through'),\n",
       " (579, 'because'),\n",
       " (566, '[Footnote:'),\n",
       " (463, 'little'),\n",
       " (419, 'should'),\n",
       " (381, 'shadow'),\n",
       " (372, 'without'),\n",
       " (358, 'Leonardo'),\n",
       " (354, 'before')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textFiles.flatMap(lambda line: line.split(\" \"))\n",
    "    \n",
    "counts = words\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .filter(lambda word: len(word[0]) > 5)\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .map(lambda word: (word[1],word[0])).sortByKey(False)\n",
    "    \n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642de69",
   "metadata": {},
   "source": [
    "№2: Найдите количество повторений последовательностей из двух слов в текстах. Выведите их в порядке убывания.\n",
    "\n",
    "[(('of', 'the'), 6900),\n",
    " (('in', 'the'), 3501),\n",
    " (('to', 'the'), 2208),\n",
    " (('and', 'the'), 1637),\n",
    " (('on', 'the'), 1618),\n",
    " (('from', 'the'), 1231)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68a2eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(10929, ('', '')),\n",
       " (6900, ('of', 'the')),\n",
       " (3501, ('in', 'the')),\n",
       " (2208, ('to', 'the')),\n",
       " (1637, ('and', 'the')),\n",
       " (1618, ('on', 'the')),\n",
       " (1231, ('from', 'the')),\n",
       " (1157, ('of', 'a')),\n",
       " (1068, ('by', 'the')),\n",
       " (849, ('it', 'is'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textFiles.flatMap(lambda line: line.split(\".\"))\\\n",
    "   .map(lambda line: line.strip().split(\" \"))\\\n",
    "   .flatMap(lambda words: (tuple(word) for word in zip(words, words[1: ])))\n",
    "\n",
    "counts = words\\\n",
    "     .map(lambda word: (word, 1))\\\n",
    "     .reduceByKey(lambda x, y: x + y)\\\n",
    "     .map(lambda word: (word[1],word[0])).sortByKey(False)\n",
    "\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fa531",
   "metadata": {},
   "source": [
    "№3: Найдите самое длинное слово из тех, которые состоят только из буквенно-цифровых символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6123af60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Nationalgymnasiummuseumsanatoriumandsuspensoriumsordinaryprivatdocentge']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textFiles.flatMap(lambda line: line.split(\"-\"))\\\n",
    "        .flatMap(lambda line: line.split(\" \"))\\\n",
    "        .flatMap(lambda line: line.split(\"—\"))\\\n",
    "\n",
    "\n",
    "counts = words\\\n",
    "     .map(lambda word: (word, 1))\\\n",
    "     .reduceByKey(lambda x, y: x + y)\\\n",
    "     .map(lambda word: (word[0])).sortBy(lambda word: len(word) * -1)\n",
    "\n",
    "counts.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037664dc",
   "metadata": {},
   "source": [
    "№4: Для каждой буквы найдите количество уникальных слов в текстах, которые на нее начинаются. Приведите буквы к верхнему регистру, отсортируйте их в алфавитном порядке.\n",
    "\n",
    "[('A', 4268), ('B', 4460), ('C', 6660), ('D', 4156), ('E', 2856), ('F', 3482)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2723bdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('A', 4943), ('B', 2990), ('C', 2675), ('D', 1843), ('E', 1405), ('F', 2148)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words  = textFiles\\\n",
    "    .filter(lambda word: len(word) > 0 and word[0].isalpha())\\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "counts = words.keys()\\\n",
    "    .map(lambda word: (word[0].upper(), 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .sortByKey()\n",
    "\n",
    "counts.take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece9dcb",
   "metadata": {},
   "source": [
    "№ 5: Откройте каталог \"/home/student/Work/Labs/3.Hadoop/Weather\" и изучите его содержимое. Здесь представлен код MapReduce для Apache Hadoop, который обрабатывает погодные данные [National Climatic Data Center](http://www.ncdc.noaa.gov/). Подробности про задачу, которую решает данная программа, прочитайте во второй главе [Hadoop: The Definitive Guide](https://grut-computing.com/HadoopBook.pdf).\n",
    "\n",
    "Погодные данные загружены в файловую систему HDFS по адресу hdfs://localhost:9000/user/student/weather. Там присутствуют данные только за 2022 год. Реализуйте решение той же задачи с погодными данными в Apache Spark с помощью методов flatMap и reduceByKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30759d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "weatherFiles = sc.wholeTextFiles('hdfs://localhost:9000/user/student/weather/*')\n",
    "\n",
    "def mapper(text):\n",
    "    ...\n",
    "\n",
    "def reducer(a, b):\n",
    "    ...\n",
    "    \n",
    "weatherFiles\\\n",
    "    .flatMap(mapper)\\\n",
    "    .reduceByKey(reducer)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323ca5b",
   "metadata": {},
   "source": [
    "№6: Решите задачу из задания 5 с помощью SQL и методов DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "weatherLines = sc.textFile('hdfs://localhost:9000/user/student/weather/*')\n",
    "weatherColumns = [\"year\",\"temperature\", \"quality\"]\n",
    "weatherDF = weatherLines\\\n",
    "    .map(lambda val: (int(val[15:19]), int(val[87:92]), int(val[92:93])))\\\n",
    "    .toDF(weatherColumns)\n",
    "\n",
    "weatherDF.createOrReplaceTempView(\"Weather\")\n",
    "\n",
    "sqlWay = spark.sql(\"\"\"\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "dataFrameWay = weatherDF\\\n",
    "    ...\n",
    "\n",
    "print(sqlWay.rdd.map(lambda x: (x[0], x[1])).collect())\n",
    "print(dataFrameWay.rdd.map(lambda x: (x[0], x[1])).collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
